{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "Question - 3 [Python - numpy].ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNl316gSSKoyAbFELVEvMdN",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/debarati-nath/10-2-class-classification-using-CIFAR10/blob/main/Question_3_%5BPython_numpy%5D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59xp8o-wurE1"
      },
      "source": [
        "Question:\n",
        "[Python (Numpy)] Implement (in Numpy) a unidirectional multi-layer LSTM classifier with input and forget gates coupled. You can find information about this variant of LSTM here (look for CIFG). The model should accept a feature vector as input and emit the corresponding posterior. Then, train a character-based language model to generate text resembling Shakespeare (use any online dataset you see fit). How do you measure the quality of the generated text? Justify all the design decisions youâ€™ve made in your training and inference pipelines."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sB_bAr_Ru5LB"
      },
      "source": [
        "#Import Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from random import uniform"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pO_3U3zjv712",
        "outputId": "20cb1d4c-d997-4e92-8f42-f8788a945f3d"
      },
      "source": [
        "#Read the file from a specific location (here, from the drive). \n",
        "data = open(\"shakespear_text.txt\", \"r\", encoding='utf-8').read() \n",
        "l_data = len(data)\n",
        "print('Length of the data:', l_data)\n",
        "print('------------\\n')\n",
        "print('Shakespeare data:\\n\\n', data[:500])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of the data: 1115393\n",
            "------------\n",
            "\n",
            "Shakespeare data:\n",
            "\n",
            " First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UA2xX1sawShq",
        "outputId": "44a9ee0b-db56-4fc3-9dc7-430456178c7d"
      },
      "source": [
        "#Lower case letter. We need to do the lower case so that the program take 'The' and 'the' as same.\n",
        "data_lowercase = data.lower()\n",
        "print('Data in lower case:', data_lowercase[:200])\n",
        "l_data_lowercase = len(data_lowercase) "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data in lower case: first citizen:\n",
            "before we proceed any further, hear me speak.\n",
            "\n",
            "all:\n",
            "speak, speak.\n",
            "\n",
            "first citizen:\n",
            "you are all resolved rather to die than to famish?\n",
            "\n",
            "all:\n",
            "resolved. resolved.\n",
            "\n",
            "first citizen:\n",
            "first, you\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GbjZNYeHwaAJ",
        "outputId": "f09ae12a-067e-4d14-ed9a-05ab5207330c"
      },
      "source": [
        "#Find out Unique character in the data\n",
        "char = sorted(set(data_lowercase))\n",
        "print(char)\n",
        "l_char = len(char)\n",
        "\n",
        "#Mapping the unique character into their indexes\n",
        "char_to_index = {char1: ind for ind, char1 in enumerate(char)}\n",
        "\n",
        "#Map character indexes to characers from the unique characters of the data\n",
        "index_to_char = {char1: ind for char1, ind in enumerate(char)}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpwctC1qw02m"
      },
      "source": [
        "Create LSTM class. I created class so that I can access the functions inside the class easily. We can also call the LSTM cell easily and more than once to optimize the losses."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqrnNMlwwySH"
      },
      "source": [
        "class LSTM:\n",
        "    def __init__(self, char_to_index, index_to_char, l_char, hidden_layer = 100, seq_length = 25, epochs = 10, lr = 0.01, beta1=0.9, beta2=0.999):\n",
        "        self.char_to_index = char_to_index\n",
        "        self.index_to_char = index_to_char\n",
        "        self.l_char = l_char\n",
        "        self.hidden_layer = hidden_layer\n",
        "        self.seq_length = seq_length\n",
        "        self.epochs = epochs\n",
        "        self.lr = lr\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        \n",
        "        #weight and bias. Used Xavier initialization is an well-known initializer\n",
        "        self.params = {}\n",
        "        Xav = 1/np.sqrt(self.l_char+self.hidden_layer)\n",
        "        \n",
        "        #Initialize the parameters\n",
        "        #cell gate\n",
        "        self.params['Wc'] = np.random.randn(self.hidden_layer, self.hidden_layer+self.l_char) * Xav\n",
        "        self.params['bc'] = np.zeros((self.hidden_layer,1))\n",
        "        #input gate\n",
        "        self.params['Wi'] = np.random.randn(self.hidden_layer, self.hidden_layer+self.l_char) * Xav\n",
        "        self.params['bi'] = np.zeros((self.hidden_layer,1))\n",
        "        #forward gate\n",
        "        self.params['Wf'] = np.random.randn(self.hidden_layer, self.hidden_layer+self.l_char) * Xav\n",
        "        self.params['bf'] = np.ones((self.hidden_layer,1))\n",
        "        #output gate\n",
        "        self.params['Wo'] = np.random.randn(self.hidden_layer, self.hidden_layer+self.l_char) * Xav\n",
        "        self.params['bo'] = np.zeros((self.hidden_layer,1))\n",
        "        \n",
        "        #output\n",
        "        self.params[\"Wv\"] = np.random.randn(self.l_char, self.hidden_layer) * (1 / np.sqrt(self.l_char))\n",
        "        self.params[\"bv\"] = np.zeros((self.l_char,1))\n",
        "        \n",
        "        #Initialize the gradients\n",
        "        self.grads = {}\n",
        "        self.adam_params = {}\n",
        "        \n",
        "        #Create key for providing spaces/locations for parameters\n",
        "        for key in self.params:\n",
        "            self.grads['d' + key] = np.zeros_like(self.params[key])\n",
        "            self.adam_params['m' + key] = np.zeros_like(self.params[key])\n",
        "            self.adam_params['v' + key] = np.zeros_like(self.params[key])\n",
        "            \n",
        "        self.smooth_loss = -np.log(1/self.l_char) * self.hidden_layer\n",
        "        return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mus3HjMex0n6"
      },
      "source": [
        "Create different function for further calling. It is easy way to create function for calling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CJm8hMdxguD"
      },
      "source": [
        "def sigmoid(self, x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "LSTM.sigmoid = sigmoid\n",
        "\n",
        "\n",
        "def tanh(self, x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "LSTM.tanh = tanh\n",
        "\n",
        "def softmax(self, x):\n",
        "    x_exp = np.exp(x - np.max(x)) \n",
        "    return x_exp / np.sum(x_exp)\n",
        "\n",
        "LSTM.softmax = softmax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGPoGNJYx9eq"
      },
      "source": [
        "Create function for forward pass"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvZf95ikxk4w"
      },
      "source": [
        "def forwardpass(self, x, h_prev, c_prev):\n",
        "    z = np.row_stack((h_prev, x))\n",
        "\n",
        "    f = self.sigmoid(np.dot(self.params[\"Wf\"], z) + self.params[\"bf\"])\n",
        "    i = self.sigmoid(np.dot(self.params[\"Wi\"], z) + self.params[\"bi\"])\n",
        "    c_bar = np.tanh(np.dot(self.params[\"Wc\"], z) + self.params[\"bc\"])\n",
        "\n",
        "    c = f * c_prev + i * c_bar\n",
        "    o = self.sigmoid(np.dot(self.params[\"Wo\"], z) + self.params[\"bo\"])\n",
        "    h = o * self.tanh(c)\n",
        "\n",
        "    v = np.dot(self.params[\"Wv\"], h) + self.params[\"bv\"]\n",
        "    y_hat = self.softmax(v)\n",
        "    return y_hat, v, h, o, c, c_bar, i, f, z\n",
        "\n",
        "LSTM.forwardpass = forwardpass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaYmuY85xvlb"
      },
      "source": [
        "Create Function for backward pass\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUxnvzGwxp-f"
      },
      "source": [
        "def backwardpass(self, y, y_hat, dh_next, dc_next, c_prev, z, f, i, c_bar, c, o, h):\n",
        "    dv = np.copy(y_hat)\n",
        "    dv[y] -= 1 \n",
        "\n",
        "    self.grads[\"dWv\"] += np.dot(dv, h.T)\n",
        "    self.grads[\"dbv\"] += dv\n",
        "\n",
        "    dh = np.dot(self.params[\"Wv\"].T, dv)\n",
        "    dh += dh_next\n",
        "    \n",
        "    do = dh * self.tanh(c)\n",
        "    da_o = do * o*(1-o)\n",
        "    self.grads[\"dWo\"] += np.dot(da_o, z.T)\n",
        "    self.grads[\"dbo\"] += da_o\n",
        "\n",
        "    dc = dh * o * (1-self.tanh(c)**2)\n",
        "    dc += dc_next\n",
        "\n",
        "    dc_bar = dc * i\n",
        "    da_c = dc_bar * (1 - c_bar**2)\n",
        "    self.grads[\"dWc\"] += np.dot(da_c, z.T)\n",
        "    self.grads[\"dbc\"] += da_c\n",
        "\n",
        "    di = dc * c_bar\n",
        "    da_i = di * i*(1-i) \n",
        "    self.grads[\"dWi\"] += np.dot(da_i, z.T)\n",
        "    self.grads[\"dbi\"] += da_i\n",
        "\n",
        "    df = dc * c_prev\n",
        "    da_f = df * f * (1 - f)\n",
        "    self.grads[\"dWf\"] += np.dot(da_f, z.T)\n",
        "    self.grads[\"dbf\"] += da_f\n",
        "\n",
        "    dz = (np.dot(self.params[\"Wf\"].T, da_f)\n",
        "         + np.dot(self.params[\"Wi\"].T, da_i)\n",
        "         + np.dot(self.params[\"Wc\"].T, da_c)\n",
        "         + np.dot(self.params[\"Wo\"].T, da_o))\n",
        "\n",
        "    dh_prev = dz[:self.hidden_layer, :]\n",
        "    dc_prev = f * dc\n",
        "    return dh_prev, dc_prev\n",
        "\n",
        "LSTM.backwardpass = backwardpass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bvt-6DIyB1K"
      },
      "source": [
        "#Create function for reset gradients. Clear gradients before the backward pass\n",
        "def reset_grads(self):\n",
        "    for key in self.grads:\n",
        "        self.grads[key].fill(0)\n",
        "    return\n",
        "\n",
        "LSTM.reset_grads = reset_grads\n",
        "\n",
        "#create a function for forward backward pass\n",
        "def forward_backward(self, x_batch, y_batch, h_prev, c_prev):\n",
        "    x, z = {}, {}\n",
        "    f, i, c_bar, c, o, y_hat, v, h = {}, {}, {}, {}, {}, {}, {}, {}\n",
        "\n",
        "    # Values at t= - 1\n",
        "    h[-1] = h_prev\n",
        "    c[-1] = c_prev\n",
        "\n",
        "    loss = 0\n",
        "    for t in range(self.seq_length): \n",
        "        x[t] = np.zeros((self.l_char, 1))\n",
        "        x[t][x_batch[t]] = 1\n",
        "\n",
        "        y_hat[t], v[t], h[t], o[t], c[t], c_bar[t], i[t], f[t], z[t] = self.forwardpass(x[t], h[t-1], c[t-1])\n",
        "\n",
        "        loss += -np.log(y_hat[t][y_batch[t], 0])\n",
        "\n",
        "    self.reset_grads()\n",
        "\n",
        "    dh_next = np.zeros_like(h[0])\n",
        "    dc_next = np.zeros_like(c[0])\n",
        "\n",
        "    for t in reversed(range(self.seq_length)):\n",
        "        dh_next, dc_next = self.backwardpass(y_batch[t], y_hat[t], dh_next, \n",
        "                                              dc_next, c[t-1], z[t], f[t], i[t], \n",
        "                                              c_bar[t], c[t], o[t], h[t]) \n",
        "    return loss, h[self.seq_length - 1], c[self.seq_length - 1]\n",
        "\n",
        "LSTM.forward_backward = forward_backward"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QopWfDEiz2wb"
      },
      "source": [
        "Sample the data for updating and displaying.\n",
        "Create a function for data sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlGywbdOzvfU"
      },
      "source": [
        "def sample(self, h_prev, c_prev, sample_size):\n",
        "    x = np.zeros((self.l_char, 1))\n",
        "    h = h_prev\n",
        "    c = c_prev\n",
        "    sample_string = \"\" \n",
        "    \n",
        "    for t in range(sample_size):\n",
        "        y_hat, _, h, _, c, _, _, _, _ = self.forwardpass(x, h, c)        \n",
        "        \n",
        "        # Evaluate a random index within the probability distribution of y_hat(ravel())\n",
        "        index = np.random.choice(range(self.l_char), p=y_hat.ravel())\n",
        "        x = np.zeros((self.l_char, 1))\n",
        "        x[index] = 1\n",
        "        \n",
        "        #find the char with the sampled index and concat to the output string\n",
        "        chars = self.index_to_char[index]\n",
        "        sample_string += chars\n",
        "    return sample_string\n",
        "\n",
        "LSTM.sample = sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ns6HcriPz1nf"
      },
      "source": [
        "Need to clip gradiant to overcome the gradient exploding\n",
        "Create a function for gradient clipping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnRCbwPb0NNz"
      },
      "source": [
        "def clip_grads(self):\n",
        "    for key in self.grads:\n",
        "        np.clip(self.grads[key], -1, 1, out=self.grads[key])\n",
        "    return\n",
        "\n",
        "LSTM.clip_grads = clip_grads"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWEpvSK50RX6"
      },
      "source": [
        "#Create function for gradient updating\n",
        "def update_params(self, batch_num):\n",
        "    for key in self.params:\n",
        "        self.adam_params[\"m\"+key] = self.adam_params[\"m\"+key] * self.beta1 + \\\n",
        "                                    (1 - self.beta1) * self.grads[\"d\"+key]\n",
        "        self.adam_params[\"v\"+key] = self.adam_params[\"v\"+key] * self.beta2 + \\\n",
        "                                    (1 - self.beta2) * self.grads[\"d\"+key]**2\n",
        "\n",
        "        m_correlated = self.adam_params[\"m\" + key] / (1 - self.beta1**batch_num)\n",
        "        v_correlated = self.adam_params[\"v\" + key] / (1 - self.beta2**batch_num) \n",
        "        self.params[key] -= self.lr * m_correlated / (np.sqrt(v_correlated) + 1e-8) \n",
        "    return\n",
        "\n",
        "LSTM.update_params = update_params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxCUyBPo0f88"
      },
      "source": [
        "#Create a function for gradient checking\n",
        "def gradient_check(self, x_batch, y_batch, h_prev, c_prev, num_checks=10, delta=1e-6):\n",
        "        print(\"Gradient check...\\n\")\n",
        "\n",
        "        _, _, _ = self.forward_backward(x_batch, y_batch, h_prev, c_prev)\n",
        "        grads_numerical = self.grads\n",
        "\n",
        "        for key in self.params:\n",
        "            print(\"<---------\", key, \"--------->\")\n",
        "            test = True\n",
        "\n",
        "            dims = self.params[key].shape\n",
        "            grad_numerical = 0\n",
        "            grad_analytical = 0\n",
        "            \n",
        "            #Used 10 samples\n",
        "            for _ in range(num_checks):  \n",
        "                \n",
        "                #Length\n",
        "                idx = int(uniform(0, self.params[key].size))\n",
        "                old_val = self.params[key].flat[idx]\n",
        "                \n",
        "                #x = x + delta\n",
        "                self.params[key].flat[idx] = old_val + delta\n",
        "                J_plus, _, _ = self.forward_backward(x_batch, y_batch, h_prev, c_prev)\n",
        "                \n",
        "                #x = x- delta\n",
        "                self.params[key].flat[idx] = old_val - delta\n",
        "                J_minus, _, _ = self.forward_backward(x_batch, y_batch, h_prev, c_prev)\n",
        "                \n",
        "                #reset\n",
        "                self.params[key].flat[idx] = old_val\n",
        "\n",
        "                grad_numerical += (J_plus - J_minus) / (2 * delta)             #Numerical calculation\n",
        "                grad_analytical += grads_numerical[\"d\" + key].flat[idx]        #Analytical calculation\n",
        "\n",
        "            grad_numerical /= num_checks\n",
        "            grad_analytical /= num_checks\n",
        "            \n",
        "            #Evaluation error\n",
        "            rel_error = abs(grad_analytical - grad_numerical) / abs(grad_analytical + grad_numerical)\n",
        "\n",
        "         \n",
        "            #Fixed limit to stop evaluation for each parameter \n",
        "            if rel_error > 1e-2:\n",
        "                if not (grad_analytical < 1e-6 and grad_numerical < 1e-6):\n",
        "                        test = False\n",
        "                        assert (test)\n",
        "     \n",
        "                 \n",
        "            print('Numerical gradient: \\t%e, Analytical gradient: \\t%e =>  Error: \\t%e' % (grad_numerical, grad_analytical, rel_error))\n",
        "      \n",
        "        return\n",
        "\n",
        "    \n",
        "LSTM.gradient_check = gradient_check"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skH4HmUg04NR"
      },
      "source": [
        "def train(self, X, verbose=True):\n",
        "    #Store losses\n",
        "    J = []  \n",
        "    #Length of batches\n",
        "    num_batches = len(X) // self.seq_length\n",
        "    \n",
        "    # Trim input to have full sequences\n",
        "    X_trimmed = X[: num_batches * self.seq_length]  \n",
        "   \n",
        "    \n",
        "\n",
        "    for epoch in range(self.epochs):\n",
        "        h_prev = np.zeros((self.hidden_layer, 1))\n",
        "        c_prev = np.zeros((self.hidden_layer, 1))\n",
        "\n",
        "        for j in range(0, len(X_trimmed) - self.seq_length, self.seq_length):\n",
        "            # prepare batches for training\n",
        "            x_batch = [self.char_to_index[ch] for ch in X_trimmed[j: j + self.seq_length]]\n",
        "            y_batch = [self.char_to_index[ch] for ch in X_trimmed[j + 1: j + self.seq_length + 1]]\n",
        "\n",
        "            loss, h_prev, c_prev = self.forward_backward(x_batch, y_batch, h_prev, c_prev)\n",
        "\n",
        "            # Smooth out loss and store in list\n",
        "            self.smooth_loss = self.smooth_loss * 0.999 + loss * 0.001\n",
        "            J.append(self.smooth_loss)\n",
        "\n",
        "             # Check gradients\n",
        "            if epoch == 0 and j == 0:\n",
        "                self.gradient_check(x_batch, y_batch, h_prev, c_prev, 10, 1e-7)\n",
        "            \n",
        "            #Gradient clipping\n",
        "            self.clip_grads()\n",
        "            \n",
        "            #Update gradient\n",
        "            batch_num = epoch * self.epochs + j / self.seq_length + 1\n",
        "            self.update_params(batch_num)\n",
        "\n",
        "            # Print out loss and sample string\n",
        "            if verbose:\n",
        "                if j % 400000 == 0:\n",
        "                    print('Epoch:', epoch, '\\tBatch:', j, \"-\", j + self.seq_length,\n",
        "                          '\\tLoss:', round(self.smooth_loss, 2))\n",
        "                    s = self.sample(h_prev, c_prev, sample_size=250)\n",
        "                    print(s, \"\\n\")\n",
        "                    \n",
        "            \n",
        "    return J, self.params\n",
        "\n",
        "LSTM.train = train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyjvEt4G4Y5B"
      },
      "source": [
        "Call the LSTM class as model. Then, training the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SciulrFi16Hc",
        "outputId": "881c7b11-33d6-41f5-b832-e8bfb2533eff"
      },
      "source": [
        "model = LSTM(char_to_index, index_to_char, l_char, epochs = 10, lr = 0.01)\n",
        "\n",
        "J, params = model.train(data_lowercase)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient check...\n",
            "\n",
            "<--------- Wc --------->\n",
            "Numerical gradient: \t8.981466e-03, Analytical gradient: \t8.981473e-03 =>  Error: \t3.972796e-07\n",
            "<--------- bc --------->\n",
            "Numerical gradient: \t4.837937e-02, Analytical gradient: \t4.837929e-02 =>  Error: \t8.062746e-07\n",
            "<--------- Wi --------->\n",
            "Numerical gradient: \t1.375042e-04, Analytical gradient: \t1.375900e-04 =>  Error: \t3.118371e-04\n",
            "<--------- bi --------->\n",
            "Numerical gradient: \t5.030564e-03, Analytical gradient: \t5.030541e-03 =>  Error: \t2.344163e-06\n",
            "<--------- Wf --------->\n",
            "Numerical gradient: \t5.993641e-04, Analytical gradient: \t5.993248e-04 =>  Error: \t3.279500e-05\n",
            "<--------- bf --------->\n",
            "Numerical gradient: \t1.649120e-02, Analytical gradient: \t1.649116e-02 =>  Error: \t1.122479e-06\n",
            "<--------- Wo --------->\n",
            "Numerical gradient: \t-7.998580e-04, Analytical gradient: \t-7.998698e-04 =>  Error: \t7.407268e-06\n",
            "<--------- bo --------->\n",
            "Numerical gradient: \t4.297313e-03, Analytical gradient: \t4.297312e-03 =>  Error: \t1.341291e-07\n",
            "<--------- Wv --------->\n",
            "Numerical gradient: \t-1.094682e-02, Analytical gradient: \t-1.094683e-02 =>  Error: \t4.007670e-07\n",
            "<--------- bv --------->\n",
            "Numerical gradient: \t5.513069e-01, Analytical gradient: \t5.513069e-01 =>  Error: \t6.820687e-08\n",
            "Epoch: 0 \tBatch: 0 - 25 \tLoss: 366.08\n",
            "?yjbp!tmsrrbiyvqkcd&h:jh!,,-obw jqpufivgfl;l-nyethkse.k\n",
            "e?lw:!gz ol jdfd$soto hth?$e$kp&wju&cbwv3r' ':s?cqqsrxh!!k-.fu!q??;trjhghbu; h$-ez-'zp,ia!ppa!f?$,$&n,bvfx3a3v$\n",
            "esehzgebxjyvlo,3juwbp!cqektclb'b\n",
            "ci!ozbkn,!$!.ozrzn,x.ueufzhzuqgjrkv?gpi,anizfvpu' \n",
            "\n",
            "Epoch: 0 \tBatch: 400000 - 400025 \tLoss: 40.21\n",
            "ion\n",
            "its rumprice, shall so him kice you,\n",
            "is is with their sealserf?\n",
            "and and bose you grave.\n",
            "\n",
            "dude,\n",
            "and come their tarled,\n",
            "of lositorsing metreysing of night blows:\n",
            "flatts of low into the choksiotoo.\n",
            "farth?\n",
            "\n",
            "lad my king:\n",
            "friends spighon, with of brasi \n",
            "\n",
            "Epoch: 0 \tBatch: 800000 - 800025 \tLoss: 41.24\n",
            "aness and by the luck all they thim;\n",
            "they word belovel thence un's nothing than minf\n",
            "more.\n",
            "\n",
            "pay\n",
            "cuck\n",
            "what not was you's yet in patteral\n",
            "of wearce than newted greats mo,\n",
            "thet stwite untay you fry have by your hand,\n",
            "goibr by the day heads of fit.\n",
            "\n",
            "clow \n",
            "\n",
            "Epoch: 1 \tBatch: 0 - 25 \tLoss: 39.68\n",
            ", nor to but should?\n",
            "full them:\n",
            "no, thinks than thee, thatey of such to raction them.\n",
            "\n",
            "sebastian:\n",
            "for which the friapon;\n",
            "there i, dread! should his make;--ave with unoacles and water in upon they?\n",
            "\n",
            "sebastian:\n",
            "ay, byself.\n",
            "\n",
            "antonio:\n",
            "which spress---eyed \n",
            "\n",
            "Epoch: 1 \tBatch: 400000 - 400025 \tLoss: 38.76\n",
            " heel in mine.\n",
            "\n",
            "king richard ii:\n",
            "alt were ofucged go?\n",
            "\n",
            "bushy:\n",
            "the hill rislet world go liver.\n",
            "\n",
            "firertourlanoid\n",
            "sim sgries replai tear?\n",
            "\n",
            "unlimbiors:\n",
            "alcopship their listt losious sedies' carp,\n",
            "and made doth bolingbroke weep up!\n",
            "go, esfer else,\n",
            "that wh \n",
            "\n",
            "Epoch: 1 \tBatch: 800000 - 800025 \tLoss: 40.95\n",
            "\n",
            "is you,\n",
            "and call hums he, neathe not her us deder.\n",
            "\n",
            "florizel:\n",
            "and nose,\n",
            "by yourselvon is my she of my child,\n",
            "i have being\n",
            "into in't:\n",
            "joy; does firly with\n",
            "shall not of elp all;\n",
            "but she have dear of, and soft dreet hand\n",
            "is lapred s'rexlatt be them\n",
            "was \n",
            "\n",
            "Epoch: 2 \tBatch: 0 - 25 \tLoss: 39.82\n",
            "xe\n",
            "sletter\n",
            "her dreath again person and servel to your can'rothe kind son s neckmer.\n",
            "\n",
            "sebastenoy:\n",
            "sir, or swikeds, when a done.\n",
            "pratty spler\n",
            "most o' the sebasting thou i seel.\n",
            "\n",
            "antonio:\n",
            "swesting. not heavy; the star.\n",
            "\n",
            "antonio:\n",
            "you meat the at home: fo \n",
            "\n",
            "Epoch: 2 \tBatch: 400000 - 400025 \tLoss: 39.61\n",
            "\n",
            "the the must do gid where labrosmed the hands beak you fair go:\n",
            "peas whilst the cousin.\n",
            "\n",
            "henry booon:\n",
            "\n",
            "duke of the pappredr:\n",
            "this is.\n",
            "\n",
            "pale:\n",
            "gions:\n",
            "i progasul: we have my life you you their hand, if keen,\n",
            "and with what doubled so:\n",
            "sing me it: i well \n",
            "\n",
            "Epoch: 2 \tBatch: 800000 - 800025 \tLoss: 41.69\n",
            " are a well as and have know it i do life that the crown; i'll go\n",
            "holse.\n",
            "\n",
            "fliest:\n",
            "chate these love the gream: go of and must i roise; that blheped, and have that what myself done the love.\n",
            "\n",
            "pine:\n",
            "i am your blasce, all: whit? i'll went alchable, and b \n",
            "\n",
            "Epoch: 3 \tBatch: 0 - 25 \tLoss: 40.83\n",
            "xtawn:\n",
            "my son it speak,\n",
            "who shall not perlighty eyes;--\n",
            "and still my neight and mean, the foul nature wilt strouts\n",
            "it curst be that to say\n",
            "agest it it heavy it me, so to a to it than?\n",
            "\n",
            "antonio:\n",
            "i a should speak it would for a sir, wheres in apout! he \n",
            "\n",
            "Epoch: 3 \tBatch: 400000 - 400025 \tLoss: 39.96\n",
            "xind flress, which what the heart you our her waused\n",
            "their shot up doth we dare, england her king an gry doth rry: if in remember\n",
            "the met flower.\n",
            "\n",
            "repais:\n",
            "her chire, lean on and to have forth's colight:\n",
            "i'll go true blat thou go;\n",
            "go you eye her, frie \n",
            "\n",
            "Epoch: 3 \tBatch: 800000 - 800025 \tLoss: 42.07\n",
            "xres.\n",
            "\n",
            "florp:\n",
            "you'll not to her. my minds: was killiatar from this;\n",
            "but\n",
            "jancing uncreth at hath mations of her of the hoar it soon vicgity her.\n",
            "\n",
            "polixenes:\n",
            "he have nogtheen she, gods?\n",
            "\n",
            "plifther:\n",
            "your pessing; you with thoughts be ration: and, sore\n",
            "pr \n",
            "\n",
            "Epoch: 4 \tBatch: 0 - 25 \tLoss: 40.97\n",
            "xrne\n",
            "the body.\n",
            "\n",
            "semia:\n",
            "yet it, 'tis. she forth getture she both-ag; other neight no got your house\n",
            "aelver, with, sebastion striking it camits and worlds and it do sebasties a bring, and our good,\n",
            "fveve.\n",
            "\n",
            "sebastian:\n",
            "the tatined uses the graven hope th \n",
            "\n",
            "Epoch: 4 \tBatch: 400000 - 400025 \tLoss: 40.01\n",
            ".\n",
            "his a lord,\n",
            "of thy out of veptin with his lord up shall behol plainks and shrumbroit think in thus cursed with casted hurd, sweeth '\n",
            "harlicing madam, true should uncless himself\n",
            "tholemence of inteomils;\n",
            "and little,\n",
            "the heats, for her well\n",
            "arrow.\n",
            "\n",
            "g \n",
            "\n",
            "Epoch: 4 \tBatch: 800000 - 800025 \tLoss: 42.29\n",
            "xoss should decks viens,\n",
            "ever\n",
            "and faulath he your service.\n",
            "\n",
            "father:\n",
            "yer\n",
            "neory as tise breas and not of reward? not, he any\n",
            "off sinel: litide, notder. but known? foils the wound light.\n",
            "know thou being peffest\n",
            "than he: known it his swelrre of you forch \n",
            "\n",
            "Epoch: 5 \tBatch: 0 - 25 \tLoss: 41.21\n",
            "quy.\n",
            "\n",
            "sebassin:\n",
            "no; she bh ship heaven spa-fluslet air--\n",
            "so i do they do will ssel,\n",
            "mality.\n",
            "\n",
            "dien:\n",
            "into these this when hang my fathor,\n",
            "that i so wengines it in her metity?\n",
            "it to frope it\n",
            "with\n",
            "thee.\n",
            "\n",
            "edslias:\n",
            "sir,\n",
            "then host:\n",
            "so father and preseity: i \n",
            "\n",
            "Epoch: 5 \tBatch: 400000 - 400025 \tLoss: 40.32\n",
            "\n",
            "flather brave do\n",
            "again to hisself, hourl on fored forth up forews.\n",
            "\n",
            "greet:\n",
            "their cread counturing cure, gentle lady\n",
            "friends is and the condempation of her\n",
            "shall shall we so so sullition.\n",
            "\n",
            "hene dearallowle:\n",
            "\n",
            "henry phoo:\n",
            "why, her speak\n",
            "'compoid far an \n",
            "\n",
            "Epoch: 5 \tBatch: 800000 - 800025 \tLoss: 42.56\n",
            "xterning\n",
            "and of\n",
            "her,\n",
            "and now save us he's.\n",
            "\n",
            "flore:\n",
            "he bast\n",
            "the better.\n",
            "\n",
            "think a dance.\n",
            "\n",
            "yourselves:\n",
            "and prize\n",
            "a news?\n",
            "\n",
            "cormis:\n",
            "so she between\n",
            "than beg-rimy; or as must hand good more\n",
            "cannot prime\n",
            "you ever my ment,\n",
            "ment.\n",
            "\n",
            "morne:\n",
            "and membrack: now!\n",
            "\n",
            "fl \n",
            "\n",
            "Epoch: 6 \tBatch: 0 - 25 \tLoss: 41.7\n",
            "xton:\n",
            "thy trick shiars.\n",
            "\n",
            "antonio:\n",
            "let't not he think, no stangon a reading\n",
            "as have shall not do no bosts: and  one remember who mires.\n",
            "\n",
            "sebastian:\n",
            "no dho.\n",
            "no, and i have\n",
            "and then?\n",
            "\n",
            "gonzalo:\n",
            "thou lightert, pozen.\n",
            "\n",
            "sebastian:\n",
            "a dreadness duty lape\n",
            "a pe \n",
            "\n",
            "Epoch: 6 \tBatch: 400000 - 400025 \tLoss: 40.81\n",
            "!\n",
            "\n",
            "henry gard:\n",
            "i'll that exprey,\n",
            "and was and her not keep the? head,\n",
            "for up thine\n",
            "'taurhy, loodetse \n",
            "king richard id:\n",
            "go whor staunth and state hath stand man tis rade the pasies:\n",
            "aed deposeth than there thronder that not us \n",
            "rus,\n",
            "mine,\n",
            "'if;\n",
            "or tende \n",
            "\n",
            "Epoch: 6 \tBatch: 800000 - 800025 \tLoss: 43.19\n",
            "xen\n",
            "than sting nor my backs of pastral is haich your for's sides? hand-own; and can he mingle this?\n",
            "\n",
            "polixenes:\n",
            "does never hath inlutter sugresh:\n",
            "and love man? which! when\n",
            "faulting jusags,\n",
            "falge;\n",
            "if\n",
            "it a even 'noble own not. never. but prozan,-honort \n",
            "\n",
            "Epoch: 7 \tBatch: 0 - 25 \tLoss: 41.98\n",
            "xtaughfuding aspesser: i\n",
            "dosle\n",
            "their venkniveant doth spoke, i' the it?\n",
            "\n",
            "vincolour:\n",
            "i goverstrest that strain curse two place and ever a dear-failiss; who a kny. let eusud it?\n",
            "\n",
            "antonio:\n",
            "sir, ere that i\n",
            "so;\n",
            "i would thank were moke. what place\n",
            "at an yo \n",
            "\n",
            "Epoch: 7 \tBatch: 400000 - 400025 \tLoss: 41.07\n",
            "\n",
            "everted;\n",
            "when meet, cousin, cras cait bill\n",
            "chambh hidward of suck gracted:\n",
            "thou hast diraked plups,\n",
            "'twill as had richard show'd depiting sprime the king pain?\n",
            "give his heaven,\n",
            "wherein bring.\n",
            "\n",
            "henry whot:\n",
            "my thy king: i'll rim borth, despair: flesho \n",
            "\n",
            "Epoch: 7 \tBatch: 800000 - 800025 \tLoss: 43.29\n",
            "xtme robs.\n",
            "\n",
            "floud, his if i say? and as you happile leaved.\n",
            "\n",
            "from:\n",
            "he betten widey to daughtel;\n",
            "than how guo hall for her this a\n",
            "much soures?\n",
            "\n",
            "thrnk:\n",
            "fick henry in that framiss somerse,\n",
            "whith inficus\n",
            "of?\n",
            "\n",
            "florizel:\n",
            "who, he is?\n",
            "\n",
            "lords:\n",
            "what?\n",
            "\n",
            "pernant  \n",
            "\n",
            "Epoch: 8 \tBatch: 0 - 25 \tLoss: 42.38\n",
            "zonas,\n",
            "potititian stroke.\n",
            "\n",
            "gonzalo:\n",
            "i say our prize; the dieved the \n",
            "minnas:?\n",
            "\n",
            "fromso:\n",
            "would throw-sem\n",
            "the sake signdes visy, undian daughter on't shuck any thanience,\n",
            "and the in at couse take thee\n",
            "where's inful on your visies sir, as other it five.\n",
            " \n",
            "\n",
            "Epoch: 8 \tBatch: 400000 - 400025 \tLoss: 41.51\n",
            "\n",
            "rawn falterbed is weight:\n",
            "alterl chanfed and he had wouldir reward lady cuiced:\n",
            "wherefores shealt\n",
            "he is, and, they hear light'd?\n",
            "ah, if her as grag up bolingbroke him, dead\n",
            "up the sire the shaped!\n",
            "\n",
            "her:z:\n",
            "and while? open' well.\n",
            "\n",
            "hand:\n",
            "nay, man 'heir \n",
            "\n",
            "Epoch: 8 \tBatch: 800000 - 800025 \tLoss: 43.82\n",
            "xque a hand;\n",
            "some: hand,\n",
            "his father none? fought of him\n",
            "swing fault:\n",
            "pledion to such what and dear, and a better\n",
            "so the dead\n",
            " jogse pore?\n",
            "\n",
            "clown:\n",
            "and the is should i say of so,\n",
            "to up;\n",
            "than phys, faith, i'll no dods know a butientic? to mad, what\n",
            "belo \n",
            "\n",
            "Epoch: 9 \tBatch: 0 - 25 \tLoss: 42.89\n",
            "zzage;\n",
            "thou? i may his save resy me. we i master over me sacest would nothing sebian?.\n",
            "\n",
            "sebastian:\n",
            "i do \n",
            "pro:\n",
            "foltonchn-souls: my sebassisting of here.\n",
            "\n",
            "sebastian:\n",
            "the cartions the kind all on you, and each, commonior like. prozater?\n",
            "sirh fower-bllet \n",
            "\n",
            "Epoch: 9 \tBatch: 400000 - 400025 \tLoss: 42.0\n",
            "\n",
            "me of greed law.\n",
            "'twell interful canst profurred death'd:\n",
            "what lad in he hath my king richard's that the looks guestly purse'tly:\n",
            "what sees\n",
            "the hather soeed.\n",
            "intent?\n",
            "\n",
            "wast:\n",
            "what her which do therefort\n",
            "joubles been the arming:\n",
            "and it meed; make this  \n",
            "\n",
            "Epoch: 9 \tBatch: 800000 - 800025 \tLoss: 44.46\n",
            "x him your darve as your, which\n",
            "while a death:\n",
            "to more?\n",
            "\n",
            "flos, for in done;\n",
            "call body him.\n",
            "\n",
            "polixel of his awfuras: livarrah. oath, he whips io, which perse but bilieas? whither?\n",
            "make he\n",
            "well. bent he she since make numan, he sign from me daughter?\n",
            "g \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQqtpK-s4hEx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "bbeb1708-b9c3-4524-dded-8cb40f24060d"
      },
      "source": [
        "plt.plot([i for i in range(len(J))], J)\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Total training loss over iterations')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Total training loss over iterations')"
            ]
          },
          "metadata": {},
          "execution_count": 17
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}